{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Final House Price Analysis Report\n\nThis notebook presents the complete end-to-end analysis of King County House Prices, including data cleaning, feature engineering, and model comparison (Linear Regression, Random Forest, XGBoost).\n\n## 1. Setup and Data Loading\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\nfrom pathlib import Path\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom IPython.display import Image, display\n\n# Add src to path\nsys.path.insert(0, str(Path.cwd().parent / 'src'))\n\nfrom houseprice.config import DATA_PATH, OUT_DIR, RANDOM_STATE, TEST_SIZE\nfrom houseprice.data import load_data, clean_data\nfrom houseprice.features import engineer_features\nfrom houseprice.preprocess import split_columns, make_preprocessors\nfrom houseprice.models import make_linear, make_random_forest, make_xgb\nfrom houseprice.plots import (\n    plot_lr_residuals_enhanced, \n    plot_tree_importance,\n    plot_feature_importance_comparison,\n    plot_linear_coefficients,\n    plot_shap_summary\n)\n\n# Initialize output directory\nout_dir = Path.cwd().parent / OUT_DIR\nout_dir.mkdir(parents=True, exist_ok=True)\nprint(f\"Output directory: {out_dir}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load and clean data\nraw_df = load_data(Path.cwd().parent / DATA_PATH)\nprint(f\"Raw shape: {raw_df.shape}\")\n\ndf_clean = clean_data(raw_df)\nprint(f\"Cleaned shape: {df_clean.shape}\")\n\n# Feature Engineering\ndf = engineer_features(df_clean)\nprint(f\"Engineered shape: {df.shape}\")\nprint(\"Columns:\", df.columns.tolist())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Model Training & Comparison\nWe train three models:\n1. **Linear Regression** (Baseline, scaled features)\n2. **Random Forest** (Tree ensemble)\n3. **XGBoost** (Gradient boosting)\n\nWe use 5-fold Cross-Validation to evaluate performance.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split, KFold, GridSearchCV, cross_validate\nfrom sklearn.metrics import r2_score, root_mean_squared_error\n\n# Prepare X and y\ny_log = np.log1p(df[\"price\"].values)\nX = df.drop(columns=[\"price\"])\n\n# Train/Test Split\nX_train, X_test, y_train_log, y_test_log = train_test_split(\n    X, y_log, test_size=TEST_SIZE, random_state=RANDOM_STATE\n)\n\n# Pipelines\nnum, cat = split_columns(X_train)\nprep_lr, prep_trees = make_preprocessors(num, cat)\n\nlr_pipe, lr_grid = make_linear(prep_lr)\nrf_pipe, rf_grid = make_random_forest(prep_trees, RANDOM_STATE)\nxgb_pipe, xgb_grid = make_xgb(prep_trees, RANDOM_STATE)\n\nmodels = [\n    (\"LinearRegression\", lr_pipe, lr_grid),\n    (\"RandomForest\", rf_pipe, rf_grid),\n]\nif xgb_pipe:\n    models.append((\"XGBoost\", xgb_pipe, xgb_grid))\n\n# Run CV\nkf = KFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\nbest_estimators = {}\nresults = []\n\nprint(\"Training models...\")\nfor name, pipe, grid in models:\n    print(f\"  > Tuning {name}...\")\n    gs = GridSearchCV(pipe, param_grid=grid, cv=kf, scoring=\"r2\", n_jobs=-1)\n    gs.fit(X_train, y_train_log)\n    best_estimators[name] = gs.best_estimator_\n    \n    cv_res = cross_validate(gs.best_estimator_, X_train, y_train_log, cv=kf, \n                            scoring=[\"r2\", \"neg_root_mean_squared_error\"])\n    \n    r2 = cv_res[\"test_r2\"].mean()\n    rmse = -cv_res[\"test_neg_root_mean_squared_error\"].mean()\n    \n    # Approximate dollar RMSE\n    # Note: rigorous way is expm1(pred) vs true, but this gives a quick view on log scale or we compute explicitly\n    # Let's compute explicitly for consistency with report\n    y_pred_oof = cross_validate(gs.best_estimator_, X_train, y_train_log, cv=kf, scoring=\"r2\")\n    # Actually, simpler to just trust the scripts/run_cv.py for the exact table\n    # Here we just show the results we found.\n    \n    results.append({\n        \"Model\": name,\n        \"Best Params\": str(gs.best_params_),\n        \"R2 (CV Mean)\": r2\n    })\n\nresults_df = pd.DataFrame(results)\ndisplay(results_df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. In-Depth Analysis\n\n### 3.1 Linear Regression Residuals\nLinear Regression often fails to capture non-linear market dynamics.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "lr_model = best_estimators[\"LinearRegression\"]\nplot_lr_residuals_enhanced(lr_model, X_test, y_test_log, out_dir / \"final_lr_residuals.png\")\ndisplay(Image(filename=str(out_dir / \"final_lr_residuals.png\")))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.2 Feature Importance (Random Forest vs XGBoost)\nComparing what the tree models find important.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if \"RandomForest\" in best_estimators and \"XGBoost\" in best_estimators:\n    plot_feature_importance_comparison(\n        best_estimators[\"RandomForest\"], \n        best_estimators[\"XGBoost\"], \n        prep_trees, \n        out_dir / \"final_imp_comparison.png\"\n    )\n    display(Image(filename=str(out_dir / \"final_imp_comparison.png\")))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.3 SHAP Analysis\nUnderstanding how specific feature values push the price up or down.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Using the best tree model\nwinner_name = results_df.sort_values(\"R2 (CV Mean)\", ascending=False).iloc[0][\"Model\"]\nprint(f\"Analyzing {winner_name} with SHAP...\")\n\nif winner_name in [\"RandomForest\", \"XGBoost\"]:\n    plot_shap_summary(best_estimators[winner_name], X_train, out_dir / \"final_shap.png\")\n    display(Image(filename=str(out_dir / \"final_shap.png\")))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Final Conclusions\n\n1.  **Model Performance**: Ensemble methods (RF/XGB) significantly outperform Linear Regression.\n2.  **Key Drivers**: Square footage and grade are dominant, but location (zipcode/lat/long) is critical.\n3.  **Data Quality**: Cleaning (removing bad data) and Log-transformation of price were essential steps.\n\n### Future Work\n-   Implement neighborhood clustering using Lat/Long.\n-   Add more external data (school ratings, interest rates).\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}